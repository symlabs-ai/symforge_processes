# TDD Workflow

**Subprocesso do Execution Process â€“ implementaÃ§Ã£o guiada por testes.**

**Project (exemplo)**: forgeLLMClient (SymClient + Forge SDK)
**Team**: Agent Coders (Claude Code primary)
**Symbiota de cÃ³digo (Execution/TDD - TESTES)**: tdd_coder (`process/symbiotes/tdd_coder/prompt.md`)
**Last Updated**: 2025-11-05
**Methodology**: BDD â†’ TDD (Behavior-Driven Development â†’ Test-Driven Development)

---

## ğŸ¯ TDD Philosophy  (ajuste forgeCodeAgent)

> Nota especÃ­fica para este projeto (`forgeCodeAgent`):
> Neste macroprocesso, o **tdd_coder** atua APENAS sobre testes (features BDD, step definitions e arquivos em `tests/**`).
> A implementaÃ§Ã£o e refatoraÃ§Ã£o de cÃ³digo de produÃ§Ã£o em `src/**` Ã© responsabilidade do **forge_coder** na Fase 6 (Delivery/Sprint).
> O conteÃºdo abaixo descreve o ciclo TDD completo em termos conceituais; neste projeto, o tdd_coder aplica esses princÃ­pios
> somente na camada de testes, e o forge_coder os aplica depois na camada de cÃ³digo.

### Red-Green-Refactor Cycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RED: Write failing test             â”‚
â”‚     â†“                                    â”‚
â”‚  2. GREEN: Make test pass (minimal)     â”‚
â”‚     â†“                                    â”‚
â”‚  3. REFACTOR: Improve code quality      â”‚
â”‚     â†“                                    â”‚
â”‚  4. COMMIT: Save working state          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Principles**:
- âœ… **Test First**: Escrever teste ANTES de implementar
- âœ… **Minimal Implementation**: Fazer teste passar com cÃ³digo mÃ­nimo
- âœ… **Refactor Confidence**: Melhorar cÃ³digo sabendo que testes cobrem
- âœ… **Fast Feedback**: Testes executam rÃ¡pido (< 1 segundo)

---

## ğŸ“‹ TDD Workflow (Per Feature)

## ğŸ”– IDs das Fases (para agentes/LLMs)

No contexto do ForgeProcess, as fases principais deste workflow sÃ£o referenciadas pelos IDs:

- `execution.tdd.01.selecao_tarefa` â€” **Phase 1: SeleÃ§Ã£o da Tarefa e BDD Scenarios**
- `execution.tdd.02.red` â€” **Phase 2: Test Setup (Red Phase Start)**
- `execution.tdd.03.green_tests` â€” **Phase 3: Minimal Implementation (Green Phase)**

Fases adicionais descritas neste documento (Refactor, VCR, Commit, etc.) sÃ£o subpassos conceituais dentro dessas trÃªs fases principais.

### Phase 1: SeleÃ§Ã£o da Tarefa e BDD Scenarios

**Input Principal**: `specs/roadmap/BACKLOG.md` (item de trabalho priorizado)
**Input SecundÃ¡rio**: Feature file (ex: `specs/bdd/10_forge_core/config.feature`)

**AÃ§Ãµes**:
1. âœ… **Selecionar Tarefa do BACKLOG.md:** Escolher a prÃ³xima tarefa priorizada do `BACKLOG.md`. Esta tarefa deve referenciar um ou mais cenÃ¡rios BDD a serem implementados.
2. âœ… **Revisar Feature File BDD:** Ler o(s) feature file(s) BDD completo(s) referenciado(s) pela tarefa do backlog.
3. âœ… **Entender CenÃ¡rios Given/When/Then:** Compreender os comportamentos esperados do sistema.
4. âœ… **Mapear para Testes Python:** Preparar-se para traduzir os cenÃ¡rios para o Python (pytest-bdd).
---

### Phase 2: Test Setup (Red Phase Start)

**Goal**: Preparar ambiente de testes ANTES de implementar

**AÃ§Ãµes**:
1. âœ… Criar arquivo de teste (ex: `tests/test_config.py`)
2. âœ… Setup pytest fixtures (se necessÃ¡rio)
3. âœ… Implementar step definitions (BDD steps â†’ Python functions)
4. âœ… **RODAR TESTE** â†’ Deve falhar (RED âŒ)

**Exemplo**:
```python
# tests/test_config.py
import pytest
from pytest_bdd import scenario, given, when, then

@scenario('../specs/bdd/10_forge_core/config.feature',
          'PrecedÃªncia de configuraÃ§Ã£o (env > arquivo > defaults)')
def test_config_precedence():
    pass

@given('variÃ¡veis de ambiente definidas com provider="openrouter"')
def env_with_openrouter(monkeypatch):
    monkeypatch.setenv('FORGE_PROVIDER', 'openrouter')
    return {'provider': 'openrouter'}

@given('um arquivo de configuraÃ§Ã£o com provider="echo"')
def config_file_with_echo(tmp_path):
    config = tmp_path / "config.yaml"
    config.write_text('provider: echo')
    return str(config)

@when('inicializo o Forge')
def initialize_forge(env_with_openrouter, config_file_with_echo):
    # Isso vai falhar porque ConfigService nÃ£o existe ainda
    from forgellmclient.core.config import ConfigService
    service = ConfigService(config_file=config_file_with_echo)
    return service

@then('o provedor efetivo Ã© "openrouter" (variÃ¡vel de ambiente)')
def check_provider_openrouter(initialize_forge):
    assert initialize_forge.provider == 'openrouter'

@then('o log registra "config source: environment variable"')
def check_log_source(initialize_forge, caplog):
    assert 'config source: environment variable' in caplog.text
```

**Run Test**:
```bash
pytest tests/test_config.py::test_config_precedence -v
```

**Expected Result**: âŒ **FAIL** (RED)
```
ImportError: cannot import name 'ConfigService' from 'forgellmclient.core.config'
```

**Why RED is Good**: Confirma que o teste estÃ¡ rodando e falhando pelo motivo correto.

---

### Phase 3: Minimal Implementation (Green Phase)

**Goal**: Fazer teste passar com **cÃ³digo mÃ­nimo**

**TDD Rules**:
1. âœ… Escrever APENAS cÃ³digo suficiente para passar o teste
2. âŒ NÃ£o adicionar features extras ("YAGNI" - You Aren't Gonna Need It)
3. âŒ NÃ£o otimizar prematuramente
4. âœ… Usar Forgebase patterns obrigatÃ³rios (EntityBase, UseCaseBase, etc.)

**AÃ§Ãµes**:
1. âœ… Criar classes/funÃ§Ãµes mÃ­nimas (ConfigEntity, ConfigService)
2. âœ… Implementar lÃ³gica para passar o teste (config precedence)
3. âœ… **RODAR TESTE** â†’ Deve passar (GREEN âœ…)

**Exemplo**:
```python
# src/forgellmclient/core/config.py
from forgebase import EntityBase, UseCaseBase
from pydantic_settings import BaseSettings
import os

class ConfigEntity(EntityBase):
    """Config entity (Forgebase compliance)"""
    provider: str
    config_source: str  # "environment" | "file" | "default"

class ConfigService(UseCaseBase):
    """Config service (Forgebase compliance)"""

    def __init__(self, config_file: str = None, log_service=None):
        self.config_file = config_file
        self.log_service = log_service
        self._load_config()

    def _load_config(self):
        # Precedence: env > file > default
        if os.getenv('FORGE_PROVIDER'):
            self.provider = os.getenv('FORGE_PROVIDER')
            self.config_source = 'environment variable'
        elif self.config_file and os.path.exists(self.config_file):
            # Load from file (simplified)
            import yaml
            with open(self.config_file) as f:
                data = yaml.safe_load(f)
                self.provider = data['provider']
                self.config_source = 'config file'
        else:
            self.provider = 'echo'  # default
            self.config_source = 'default'

        if self.log_service:
            self.log_service.log(f'config source: {self.config_source}')
```

**Run Test Again**:
```bash
pytest tests/test_config.py::test_config_precedence -v
```

**Expected Result**: âœ… **PASS** (GREEN)
```
tests/test_config.py::test_config_precedence PASSED [100%]
```

---

### Phase 4: Refactor (Improve Quality)

**Goal**: Melhorar cÃ³digo mantendo testes verdes

**Refactoring Targets**:
- ğŸ”§ Remover duplicaÃ§Ã£o (DRY - Don't Repeat Yourself)
- ğŸ”§ Melhorar nomes (variÃ¡veis, funÃ§Ãµes, classes)
- ğŸ”§ Extrair funÃ§Ãµes complexas
- ğŸ”§ Adicionar type hints
- ğŸ”§ Aplicar Forgebase patterns rigorosamente

**AÃ§Ãµes**:
1. âœ… Identificar code smells (duplicaÃ§Ã£o, complexidade, etc.)
2. âœ… Refatorar cÃ³digo
3. âœ… **RODAR TESTES** â†’ Devem continuar passando âœ…
4. âœ… Repetir atÃ© cÃ³digo limpo

**Exemplo (Refactoring)**:
```python
# src/forgellmclient/core/config.py (refactored)
from forgebase import EntityBase, UseCaseBase
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Literal
import os
import yaml

class ConfigEntity(EntityBase):
    """Config entity (Forgebase compliance)"""
    provider: str
    config_source: Literal["environment", "file", "default"]
    api_keys: dict[str, str] = {}

class ConfigService(UseCaseBase):
    """Config service with precedence: env > file > default"""

    def __init__(self, config_file: str | None = None, log_service=None):
        self.config_file = config_file
        self.log_service = log_service
        self.config = self._load_config()

    def _load_config(self) -> ConfigEntity:
        """Load config with precedence"""
        provider, source = self._resolve_provider()

        config = ConfigEntity(
            provider=provider,
            config_source=source
        )

        self._log_config_source(source)
        return config

    def _resolve_provider(self) -> tuple[str, str]:
        """Resolve provider with precedence: env > file > default"""
        # 1. Check environment
        if env_provider := os.getenv('FORGE_PROVIDER'):
            return env_provider, 'environment'

        # 2. Check file
        if file_provider := self._load_from_file():
            return file_provider, 'file'

        # 3. Default
        return 'echo', 'default'

    def _load_from_file(self) -> str | None:
        """Load provider from config file"""
        if not self.config_file or not os.path.exists(self.config_file):
            return None

        with open(self.config_file) as f:
            data = yaml.safe_load(f)
            return data.get('provider')

    def _log_config_source(self, source: str):
        """Log config source (if logger available)"""
        if self.log_service:
            self.log_service.log(f'config source: {source}')

    @property
    def provider(self) -> str:
        """Get effective provider"""
        return self.config.provider
```

**Run Tests After Refactoring**:
```bash
pytest tests/test_config.py -v
```

**Expected Result**: âœ… **ALL PASS** (tests still green)

**Refactoring Benefits**:
- âœ… CÃ³digo mais limpo (extraÃ­do 3 mÃ©todos privados)
- âœ… Type hints adicionados (`str | None`, `tuple[str, str]`)
- âœ… Walrus operator (`:=`) para reduzir duplicaÃ§Ã£o
- âœ… Forgebase patterns mantidos (EntityBase, UseCaseBase)

---

### Phase 5: Additional Scenarios (Iterate)

**Goal**: Implementar cenÃ¡rios restantes da feature (TDD cycle completo para cada)

**CenÃ¡rios Restantes** (config.feature):
1. âœ… PrecedÃªncia de configuraÃ§Ã£o (DONE)
2. â³ Usar arquivo quando env nÃ£o definida
3. â³ Erro quando credenciais ausentes
4. â³ Erro quando credenciais invÃ¡lidas

**Para cada cenÃ¡rio**:
1. ğŸ”´ **RED**: Escrever teste (deve falhar)
2. ğŸŸ¢ **GREEN**: Implementar cÃ³digo mÃ­nimo (teste passa)
3. ğŸ”µ **REFACTOR**: Melhorar cÃ³digo (testes continuam verdes)
4. ğŸ’¾ **COMMIT**: Salvar estado (1 cenÃ¡rio = 1 micro-commit opcional, ou 1 feature = 1 commit)

**Exemplo (CenÃ¡rio 3: Erro quando credenciais ausentes)**:

```python
# tests/test_config.py (adicionar novo teste)
@scenario('../specs/bdd/10_forge_core/config.feature',
          'Erro quando credenciais ausentes')
def test_config_missing_credentials():
    pass

@given('que nenhuma credencial foi fornecida')
def no_credentials(monkeypatch):
    monkeypatch.delenv('OPENAI_API_KEY', raising=False)
    monkeypatch.delenv('ANTHROPIC_API_KEY', raising=False)
    return {}

@when('inicializo o Forge com provedor "openrouter"')
def initialize_forge_openrouter(no_credentials):
    from forgellmclient.core.config import ConfigService
    service = ConfigService()
    service.set_provider('openrouter')  # Isso deve falhar
    return service

@then('recebo um erro do tipo ConfigurationError')
def check_configuration_error(initialize_forge_openrouter):
    with pytest.raises(ConfigurationError) as exc:
        initialize_forge_openrouter.validate_credentials()
    assert 'API key obrigatÃ³ria' in str(exc.value)
```

**Run Test**: âŒ **RED** (ConfigurationError nÃ£o existe)

**Implement**:
```python
# src/forgellmclient/core/config.py
class ConfigurationError(Exception):
    """Raised when configuration is invalid"""
    pass

class ConfigService(UseCaseBase):
    # ... (existing code)

    def validate_credentials(self):
        """Validate that required credentials are present"""
        required_keys = {
            'openai': 'OPENAI_API_KEY',
            'anthropic': 'ANTHROPIC_API_KEY',
            'openrouter': 'OPENROUTER_API_KEY',
        }

        if self.provider in required_keys:
            env_var = required_keys[self.provider]
            if not os.getenv(env_var):
                raise ConfigurationError(
                    f'API key obrigatÃ³ria para provedor {self.provider}. '
                    f'Defina a variÃ¡vel de ambiente {env_var}.'
                )
```

**Run Test Again**: âœ… **GREEN**

---

### Phase 6: VCR.py Recording (API Tests)

**Goal**: Gravar responses reais de APIs (OpenAI, Anthropic, etc.) para replay em CI

**When to Use VCR.py**:
- âœ… Testes que chamam APIs externas (OpenAI, Anthropic, LlamaCpp, etc.)
- âœ… Testes de integraÃ§Ã£o (HTTP client, provider adapters)
- âŒ Unit tests puros (sem network calls)

**Setup**:
```python
# tests/conftest.py
import pytest
import vcr

@pytest.fixture(scope='module')
def vcr_cassette():
    """VCR.py cassette fixture"""
    my_vcr = vcr.VCR(
        cassette_library_dir='tests/cassettes',
        record_mode='once',  # Grava 1x, depois replay
        match_on=['uri', 'method'],
        filter_headers=['authorization', 'api-key'],  # Redact secrets
    )
    return my_vcr
```

**Example (OpenAI Test)**:
```python
# tests/test_openai_adapter.py
import pytest
from pytest_bdd import scenario, given, when, then

@pytest.mark.vcr()  # Use VCR.py cassette
@scenario('../specs/bdd/10_forge_core/chat.feature',
          'Enviar mensagem e receber resposta')
def test_openai_chat():
    pass

@given('que o Forge estÃ¡ configurado com o provedor "openai"')
def openai_provider(vcr_cassette):
    with vcr_cassette.use_cassette('openai_chat.yaml'):
        from forgellmclient.adapters.openai import OpenAIAdapter
        adapter = OpenAIAdapter(api_key=os.getenv('OPENAI_API_KEY'))
        return adapter

@when('envio a mensagem "OlÃ¡"')
def send_message(openai_provider):
    response = openai_provider.chat(messages=[{'role': 'user', 'content': 'OlÃ¡'}])
    return response

@then('recebo uma resposta contendo "OlÃ¡"')
def check_response(send_message):
    assert 'OlÃ¡' in send_message.content or 'olÃ¡' in send_message.content.lower()
```

**First Run (Recording)**:
```bash
# Grava cassette (faz chamada real para OpenAI API, custa $$$)
OPENAI_API_KEY=sk-... pytest tests/test_openai_adapter.py --record-mode=new_episodes
```

**Result**: Cassette salvo em `tests/cassettes/openai_chat.yaml`

**Subsequent Runs (Replay)**:
```bash
# Replay cassette (custo $0, instantÃ¢neo)
pytest tests/test_openai_adapter.py
```

**Result**: âœ… Teste passa usando response gravado (sem chamada real)

---

### Phase 7: Commit (Save Working State)

**Goal**: Salvar feature implementada + testada

**Commit Guidelines**:
- âœ… **1 feature = 1 commit** (ex: "feat(config): Implement config.feature (F01)")
- âœ… Todos os cenÃ¡rios BDD passando
- âœ… VCR.py cassettes gravados (se aplicÃ¡vel)
- âœ… Forgebase patterns aplicados
- âœ… CÃ³digo refatorado

**Commit Message Format**:
```
feat(scope): Brief description

- Feature detail 1
- Feature detail 2
- BDD scenarios: X/X passing
- VCR.py cassettes: recorded (if applicable)
- Forgebase compliance: âœ…

Story Points: X
Sprint: X
Session: YYYY-MM-DD
```

**Example**:
```bash
git add src/forgellmclient/core/config.py tests/test_config.py tests/cassettes/
git commit -m "feat(config): Implement config.feature (F01) âœ…

- ConfigEntity (Forgebase EntityBase)
- ConfigService (Forgebase UseCaseBase)
- Config precedence (env > file > default)
- Credential validation (per provider)
- Custom exceptions (ConfigurationError, AuthenticationError)
- BDD scenarios: 4/4 passing
- Forgebase compliance: âœ…

Story Points: 3
Sprint: 1
Session: 2025-11-05"
```

---

## ğŸ”„ Complete TDD Cycle (Summary)

```
Feature File (BDD)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FOR EACH SCENARIO:              â”‚
â”‚                                 â”‚
â”‚  1. ğŸ”´ RED: Write failing test  â”‚
â”‚     - Setup pytest-bdd steps    â”‚
â”‚     - Run test â†’ FAIL âŒ        â”‚
â”‚                                 â”‚
â”‚  2. ğŸŸ¢ GREEN: Minimal impl      â”‚
â”‚     - Write minimum code        â”‚
â”‚     - Run test â†’ PASS âœ…        â”‚
â”‚                                 â”‚
â”‚  3. ğŸ”µ REFACTOR: Improve        â”‚
â”‚     - Clean code                â”‚
â”‚     - Run tests â†’ STILL PASS âœ… â”‚
â”‚                                 â”‚
â”‚  4. ğŸ“¼ VCR: Record (if API)     â”‚
â”‚     - First run: real API call  â”‚
â”‚     - Save cassette             â”‚
â”‚     - Next runs: replay         â”‚
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ğŸ’¾ COMMIT: 1 feature = 1 commit
    â†“
ğŸ”„ NEXT FEATURE (repeat cycle)
```

---

## ğŸ“Š TDD Metrics

### Test Coverage Targets

| Type | Coverage Target | Speed | Cost |
|------|----------------|-------|------|
| **Unit Tests** | 80%+ | < 1s | $0 |
| **Integration Tests (VCR.py)** | 60%+ | < 5s (replay) | $0 (replay) |
| **BDD Scenarios** | 100% | < 10s | $0 (CI replay) |
| **Manual E2E** | 10% | Variable | $5/month |

**Overall Target**: 80%+ code coverage (pytest-cov)

---

## ğŸš¨ TDD Anti-Patterns (Avoid)

### âŒ Anti-Pattern 1: Writing Implementation First
```python
# âŒ WRONG: Implement before test
def config_service():
    return ConfigService()  # Implementei sem ter teste

# Later: "Hmm, como testo isso?"
```

**âœ… Correct**:
```python
# âœ… RIGHT: Test first
def test_config_service():
    service = ConfigService()  # Teste falha (nÃ£o existe)
    assert service.provider == 'echo'

# Now implement to make test pass
```

---

### âŒ Anti-Pattern 2: Testing Implementation Details
```python
# âŒ WRONG: Test internal methods
def test_config_private_method():
    service = ConfigService()
    result = service._load_from_file()  # Testing private method
    assert result == 'echo'
```

**âœ… Correct**:
```python
# âœ… RIGHT: Test public behavior
def test_config_loads_from_file():
    service = ConfigService(config_file='config.yaml')
    assert service.provider == 'echo'  # Test outcome, not internals
```

---

### âŒ Anti-Pattern 3: Skipping RED Phase
```python
# âŒ WRONG: Implement and test together
class ConfigService:
    def __init__(self):
        self.provider = 'echo'

def test_config():
    assert ConfigService().provider == 'echo'  # Teste nunca falhou (RED missing)
```

**âœ… Correct**:
```python
# âœ… RIGHT: RED â†’ GREEN â†’ REFACTOR
# 1. RED: Write test first (fails - ConfigService doesn't exist)
def test_config():
    assert ConfigService().provider == 'echo'

# 2. GREEN: Implement minimal
class ConfigService:
    def __init__(self):
        self.provider = 'echo'

# 3. REFACTOR: Improve (if needed)
```

---

### âŒ Anti-Pattern 4: Too Much Code in GREEN Phase
```python
# âŒ WRONG: Over-engineering in GREEN phase
class ConfigService:
    def __init__(self):
        self.provider = 'echo'
        self.cache = {}  # YAGNI - test doesn't need this
        self.logger = Logger()  # YAGNI
        self.validators = [...]  # YAGNI
```

**âœ… Correct**:
```python
# âœ… RIGHT: Minimal code to pass test
class ConfigService:
    def __init__(self):
        self.provider = 'echo'  # Just enough to pass
```

---

## ğŸ¯ TDD Benefits (Why It Works)

### 1. Fast Feedback Loop
- âŒ **Without TDD**: Implement â†’ Deploy â†’ Bug found â†’ Fix â†’ Re-deploy (hours/days)
- âœ… **With TDD**: Test â†’ Implement â†’ Test passes (seconds)

### 2. Living Documentation
- Tests sÃ£o **executable documentation**
- Scenario describes behavior: `test_config_precedence_env_over_file()`

### 3. Refactoring Safety
- Testes garantem que refactoring nÃ£o quebra funcionalidade
- ConfianÃ§a para melhorar cÃ³digo

### 4. Design Improvement
- TDD forÃ§a **testable design** (dependency injection, interfaces)
- Resultado: CÃ³digo mais modular, Forgebase-compliant

### 5. Cost Reduction (VCR.py)
- Gravar responses reais 1x ($$)
- Replay infinitas vezes em CI ($0)
- **Savings**: $95-475/month

---

## ğŸ” Feedback de Descobertas CrÃ­ticas para Roadmap Planning

Durante o ciclo TDD, Ã© possÃ­vel descobrir novas informaÃ§Ãµes que impactam o planejamento inicial. Quando descobertas crÃ­ticas sÃ£o feitas, que afetam a arquitetura, estimativas ou o backlog, Ã© fundamental que este feedback seja formalmente canalizado de volta para a fase de **Roadmap Planning**.

### Como Escalar Descobertas CrÃ­ticas:

1.  **IdentificaÃ§Ã£o:** Durante o RED, GREEN, ou REFACTOR, se uma suposiÃ§Ã£o do planejamento se mostrar falsa, uma dependÃªncia inesperada surgir, ou a estimativa original se provar inviÃ¡vel.
2.  **DocumentaÃ§Ã£o:** Registrar a descoberta, o impacto potencial e a evidÃªncia em um ADR provisÃ³rio ou em uma nota no `progress.md` da sprint.
3.  **ComunicaÃ§Ã£o:** Alertar o Tech Lead / Product Owner imediatamente.
4.  **Re-avaliaÃ§Ã£o:** O Tech Lead / Product Owner deve acionar uma revisÃ£o do **Roadmap Planning** para:
    *   Atualizar ADRs (`specs/roadmap/adr/ADR-XXX.md`).
    *   Revisar e ajustar estimativas (`specs/roadmap/estimates.yml`).
    *   Re-priorizar tarefas ou quebrar features no `specs/roadmap/BACKLOG.md`.
    *   Convocar uma reuniÃ£o para discutir e realinhar a arquitetura se necessÃ¡rio (HLD/LLD).

**Importante:** NÃ£o prosseguir com a implementaÃ§Ã£o que contraria o planejamento aprovado sem uma revisÃ£o e ajuste formal na fase de Roadmap Planning.

---

## ğŸ“ TDD Checklist (Per Feature)

Before marking feature as DONE:

- [ ] All BDD scenarios have pytest-bdd tests
- [ ] All tests follow RED â†’ GREEN â†’ REFACTOR cycle
- [ ] Test coverage â‰¥ 80% (pytest-cov)
- [ ] VCR.py cassettes recorded (if API tests)
- [ ] Forgebase patterns applied (EntityBase, UseCaseBase, etc.)
- [ ] Code refactored (no duplication, clear names)
- [ ] All tests pass (pytest -v)
- [ ] Committed (1 feature = 1 commit)

---

## ğŸ”— Related Documents

- **BACKLOG.md**: Session-based workflow (integrates TDD cycle)
- **estimates.yml**: Story points include TDD effort
- **ADR-005**: VCR.py testing strategy
- **FEATURE_BREAKDOWN.md**: Scenarios mapped to tests

---

**Last Updated**: 2025-11-05
**Status**: Ready for Sprint 1 Implementation
**Next Action**: Apply TDD workflow to F01 (config.feature)
